{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a39ab0",
   "metadata": {},
   "source": [
    "# L1c: Unsupervised Learning and Clustering Approaches\n",
    "Fill me in. \n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this lecture, you should be able to:\n",
    "> Three learning objectives go here.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de11cd92",
   "metadata": {},
   "source": [
    "## Examples\n",
    "Today, we will use the following examples to illustrate key concepts:\n",
    "\n",
    "> [▶ Let's explore a collection of simple and expert agents](CHEME-5660-L15a-Example-Wolfram-NetworkSimulation-Fall-2025.ipynb). In this example, we build a homogeneous collection of Wolfram market agents using simple rules. These agents watch experts and mimic their actions, leading to emergent market dynamics. We analyze how the collective behavior of these agents influences market stability and price movements.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9e123",
   "metadata": {},
   "source": [
    "## Background: What is unsupervised learning and clustering?\n",
    "Unsupervised learning is a branch of machine learning that deals with unlabeled data. Its goal is to discover hidden patterns and structures without predefined target variables. One of the most common tasks in unsupervised learning is clustering.\n",
    "\n",
    "> __What is clustering?__\n",
    "> \n",
    "> __Clustering__ is an unsupervised machine learning technique that organizes data points into groups, or clusters, based on their similarities without prior knowledge of the group memberships. This method is widely used for exploratory data analysis, enabling the discovery of patterns and relationships within complex datasets.\n",
    "\n",
    "### Clustering approaches\n",
    "Today, we'll consider [the K-means algorithm](https://en.wikipedia.org/wiki/K-means_clustering), arguably the most straightforward clustering algorithm. While relatively straightforward, we'll see that [K-means](https://en.wikipedia.org/wiki/K-means_clustering) has some shortcomings. Thus, in addition to the [K-means approach](https://en.wikipedia.org/wiki/K-means_clustering) there are several other algorithms:\n",
    "* __Hierarchical clustering__ is an unsupervised machine learning technique that organizes data points into a tree-like structure of nested clusters. This allows for the identification of relationships and patterns within the dataset. This method can be implemented using two main approaches: agglomerative, which merges individual points into larger clusters, and divisive, which splits a single cluster into smaller ones.\n",
    "* __Density-based spatial clustering of applications with noise (DBSCAN)__ is a density-based clustering algorithm that groups closely packed data points while effectively identifying outliers, making it particularly useful for datasets with noise and clusters of arbitrary shapes. By defining clusters as dense regions separated by areas of lower density, DBSCAN can efficiently discover meaningful patterns in complex data distributions\n",
    "* __Gaussian mixture models (GMMs)__ are probabilistic models that represent a dataset as a combination of multiple Gaussian distributions, each characterized by its mean and covariance. This allows for the identification of underlying subpopulations within the data. This approach is useful in clustering and density estimation, providing a flexible framework for modeling complex, multimodal distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a97bc26",
   "metadata": {},
   "source": [
    "## K-means clustering\n",
    "The [K-means algorithm](https://en.wikipedia.org/wiki/K-means_clustering), originally developed by [Lloyd in the 1950s but not published until much later in 1982](https://ieeexplore.ieee.org/document/1056489), is our first example of $\\texttt{unsupervised learning}$. Suppose we have a dataset $\\mathcal{D}=\\left\\{\\mathbf{x}_{1},\\mathbf{x}_{2},\\dots,\\mathbf{x}_{n}\\in\\mathbb{R}^{m}\\right\\}$ where each $\\mathbf{x}\\in\\mathbb{R}^{m}$ is an $m$-dimensional feature vector.\n",
    "[K-means](https://en.wikipedia.org/wiki/K-means_clustering) is a popular unsupervised machine learning algorithm for clustering data points (feature vectors) $\\mathbf{x}\\in\\mathcal{D}$ into distinct a set of groups (clusters) $\\mathcal{C} = \\left\\{\\mathcal{c}_{1},\\dots,\\mathcal{c}_{K}\\right\\}$ based on _similarity_.\n",
    "\n",
    "> __What is similarity?__\n",
    "> \n",
    "> __Similarity__ refers to how _close_ data points are to each other in the feature space, i.e., how close $\\mathbf{x}_{i}$ is to $\\mathbf{x}_{j}$ using a distance of similarity measure $d(\\mathbf{x},\\mathbf{y})$. _Close features are assumed to be similar_. The most commonly used similarity measure in [K-means clustering](https://en.wikipedia.org/wiki/K-means_clustering) is [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance). However, we can use other types of measures. The choice of similarity measure can significantly impact the resulting clusters.\n",
    "\n",
    "Let's develop some pseudocode for the [K-means algorithm](https://en.wikipedia.org/wiki/K-means_clustering).\n",
    "\n",
    "__Initialize__. You give the data set $\\mathbf{x}\\in\\mathcal{D}$, which contains `n` data vectors (measurements, observations, etc) $\\mathbf{x}_{1},\\dots,\\mathbf{x}_{n}$ where each vector $\\mathbf{x}_{i}$ has `m` features, and the number and (initial) locations of K-clusters to the algorithm. Each cluster $\\mathcal{c}_{k}$ is represented by a $\\texttt{centroid}$, i.e., the mean of the set of data points in the cluster $\\left\\{\\mathbf{x}_{i}\\right\\}_{i\\in\\mathcal{c}_{k}}$. In the [K-means approach](https://en.wikipedia.org/wiki/K-means_clustering), you have to tell the algorithm how many clusters and the initial location of each cluster. This initial guess is then iteratively refined.\n",
    "* __Update__. The [K-means algorithm](https://en.wikipedia.org/wiki/K-means_clustering) employs an iterative process in which data points are assigned to the nearest cluster centroid, and the centroids are subsequently updated based on the mean of the assigned points. This process continues until a predetermined stopping criterion is satisfied.\n",
    "* __Stopping__. There are several ways the [K-means algorithm](https://en.wikipedia.org/wiki/K-means_clustering) can terminate. The stopping criteria for the [K-means clustering algorithm](https://en.wikipedia.org/wiki/K-means_clustering) include when the cluster centroids do not change significantly, when data points remain in the same clusters across iterations, or when a maximum number of iterations is reached.\n",
    "\n",
    "Let's look at an example of the [K-means algorithm](https://en.wikipedia.org/wiki/K-means_clustering) in action.\n",
    "\n",
    "> __Example:__\n",
    "> \n",
    "> [▶ Let's explore a collection of simple and expert agents](CHEME-5660-L15a-Example-Wolfram-NetworkSimulation-Fall-2025.ipynb). In this example, we build a homogeneous collection of Wolfram market agents using simple rules. These agents watch experts and mimic their actions, leading to emergent market dynamics. We analyze how the collective behavior of these agents influences market stability and price movements.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f3290a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e195e07",
   "metadata": {},
   "source": [
    "## Summary\n",
    "One concise summary sentence goes here.\n",
    "\n",
    "> __Key Takeaways:__\n",
    ">\n",
    "> Three key takeaways go here.\n",
    "\n",
    "One concide, direct concluding sentence goes here.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
